name: Performance Monitoring

on:
  schedule:
    # Run performance tests weekly on Sundays at 3 AM UTC
    - cron: "0 3 * * 0"
  push:
    branches: [main]
    paths:
      - "api/**"
      - "ai/**"
      - "docker/**"
  workflow_dispatch:
    inputs:
      duration:
        description: "Test duration in seconds"
        required: false
        default: "300"
        type: string
      users:
        description: "Number of concurrent users"
        required: false
        default: "10"
        type: string
      endpoint:
        description: "Target endpoint"
        required: false
        default: "staging"
        type: choice
        options:
          - staging
          - production
          - local

env:
  PYTHON_VERSION: "3.11"

jobs:
  # =============================================================================
  # LOAD TESTING
  # =============================================================================

  load-testing:
    name: Load Testing
    runs-on: ubuntu-latest
    strategy:
      matrix:
        test-type: [api-only, full-pipeline, stress-test]
        include:
          - test-type: api-only
            users: 20
            duration: 300
            description: "API endpoints without AI processing"
          - test-type: full-pipeline
            users: 5
            duration: 600
            description: "Full audio transcription pipeline"
          - test-type: stress-test
            users: 50
            duration: 180
            description: "Stress test with high concurrency"
    steps:
      - name: Checkout code
        uses: actions/checkout@v5

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install performance testing tools
        run: |
          pip install locust pytest-benchmark requests numpy pandas matplotlib
          cd tests && pip install -r requirements.txt

      - name: Set target endpoint
        run: |
          case "${{ github.event.inputs.endpoint || 'staging' }}" in
            production)
              echo "TARGET_URL=https://api.pocwhisp.com" >> $GITHUB_ENV
              ;;
            staging)
              echo "TARGET_URL=https://staging.pocwhisp.com" >> $GITHUB_ENV
              ;;
            local)
              echo "TARGET_URL=http://localhost:8080" >> $GITHUB_ENV
              ;;
          esac

      - name: Start local environment (if testing locally)
        if: github.event.inputs.endpoint == 'local'
        run: |
          cd docker
          cp env.example .env
          echo "WHISPER_MODEL=tiny" >> .env
          docker-compose -f docker-compose.dev.yml up -d

          # Wait for services to be ready
          sleep 60

      - name: Generate test audio files
        run: |
          cd tests
          python create_test_audio.py --count=10 --duration=5,10,30

      - name: Run load tests - ${{ matrix.description }}
        run: |
          cd tests
          python performance_test.py \
            --test-type=${{ matrix.test-type }} \
            --endpoint=${{ env.TARGET_URL }} \
            --users=${{ github.event.inputs.users || matrix.users }} \
            --duration=${{ github.event.inputs.duration || matrix.duration }} \
            --output-dir=results/${{ matrix.test-type }}
        timeout-minutes: 20

      - name: Cleanup local environment
        if: github.event.inputs.endpoint == 'local'
        run: |
          cd docker
          docker-compose -f docker-compose.dev.yml down -v

      - name: Upload performance results
        uses: actions/upload-artifact@v3
        with:
          name: performance-results-${{ matrix.test-type }}
          path: tests/results/${{ matrix.test-type }}/

  # =============================================================================
  # BENCHMARK TESTING
  # =============================================================================

  benchmark-testing:
    name: Benchmark Testing
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v5

      - name: Set up Go
        uses: actions/setup-go@v4
        with:
          go-version: "1.21"

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          # Go dependencies
          cd api && go mod download

          # Python dependencies
          cd ../ai
          pip install -r requirements.txt
          pip install pytest-benchmark

      - name: Run Go benchmarks
        run: |
          cd api
          go test -bench=. -benchmem -benchtime=30s ./... > ../benchmark-go.txt
          cat ../benchmark-go.txt

      - name: Run Python benchmarks
        run: |
          cd ai
          python -m pytest tests/test_benchmarks.py --benchmark-only \
            --benchmark-json=../benchmark-python.json \
            --benchmark-histogram=../benchmark-histogram
        env:
          DEVICE: cpu
          WHISPER_MODEL: tiny

      - name: Generate benchmark report
        run: |
          python tests/generate_benchmark_report.py \
            --go-results=benchmark-go.txt \
            --python-results=benchmark-python.json \
            --output=benchmark-report.html

      - name: Upload benchmark results
        uses: actions/upload-artifact@v3
        with:
          name: benchmark-results
          path: |
            benchmark-*.txt
            benchmark-*.json
            benchmark-*.html
            benchmark-histogram/

  # =============================================================================
  # MEMORY PROFILING
  # =============================================================================

  memory-profiling:
    name: Memory Profiling
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v5

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install profiling tools
        run: |
          pip install memory-profiler psutil py-spy
          cd ai && pip install -r requirements.txt

      - name: Profile memory usage - Audio Processing
        run: |
          cd tests
          python memory_profile_test.py --component=audio-processing \
            --output=memory-profile-audio.txt

      - name: Profile memory usage - Model Loading
        run: |
          cd tests
          python memory_profile_test.py --component=model-loading \
            --output=memory-profile-models.txt
        env:
          DEVICE: cpu
          WHISPER_MODEL: tiny

      - name: Generate memory report
        run: |
          cd tests
          python generate_memory_report.py \
            --profiles=memory-profile-*.txt \
            --output=memory-report.html

      - name: Upload memory profiling results
        uses: actions/upload-artifact@v3
        with:
          name: memory-profiling-results
          path: |
            tests/memory-profile-*.txt
            tests/memory-report.html

  # =============================================================================
  # GPU PERFORMANCE TESTING
  # =============================================================================

  gpu-performance:
    name: GPU Performance Testing
    runs-on: [self-hosted, gpu] # Requires GPU runner
    if: github.event_name == 'schedule' || github.event_name == 'workflow_dispatch'
    steps:
      - name: Checkout code
        uses: actions/checkout@v5

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install CUDA dependencies
        run: |
          pip install torch torchaudio --index-url https://download.pytorch.org/whl/cu118
          cd ai && pip install -r requirements.txt

      - name: Check GPU availability
        run: |
          python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}')"
          python -c "import torch; print(f'GPU count: {torch.cuda.device_count()}')"
          nvidia-smi

      - name: Run GPU performance tests
        run: |
          cd tests
          python gpu_performance_test.py \
            --models=tiny,base,small,medium \
            --batch-sizes=1,2,4,8 \
            --output=gpu-performance-results.json
        env:
          DEVICE: cuda

      - name: Generate GPU performance report
        run: |
          cd tests
          python generate_gpu_report.py \
            --results=gpu-performance-results.json \
            --output=gpu-performance-report.html

      - name: Upload GPU performance results
        uses: actions/upload-artifact@v3
        with:
          name: gpu-performance-results
          path: |
            tests/gpu-performance-*.json
            tests/gpu-performance-*.html

  # =============================================================================
  # SCALABILITY TESTING
  # =============================================================================

  scalability-testing:
    name: Scalability Testing
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v5

      - name: Set up Docker
        uses: docker/setup-buildx-action@v3

      - name: Build test images
        run: |
          cd docker
          docker build -f Dockerfile.api -t pocwhisp-api:test ..
          docker build -f Dockerfile.ai --target cpu-only -t pocwhisp-ai:test ..

      - name: Run scalability tests
        run: |
          cd tests
          python scalability_test.py \
            --api-replicas=1,2,3 \
            --ai-replicas=1,2 \
            --concurrent-requests=10,20,50 \
            --output=scalability-results.json

      - name: Generate scalability report
        run: |
          cd tests
          python generate_scalability_report.py \
            --results=scalability-results.json \
            --output=scalability-report.html

      - name: Upload scalability results
        uses: actions/upload-artifact@v3
        with:
          name: scalability-results
          path: |
            tests/scalability-*.json
            tests/scalability-*.html

  # =============================================================================
  # PERFORMANCE REGRESSION TESTING
  # =============================================================================

  regression-testing:
    name: Performance Regression
    runs-on: ubuntu-latest
    if: github.event_name == 'push'
    steps:
      - name: Checkout code
        uses: actions/checkout@v5

      - name: Checkout previous version
        uses: actions/checkout@v5
        with:
          ref: ${{ github.event.before }}
          path: previous

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          pip install pytest-benchmark requests pandas
          cd tests && pip install -r requirements.txt

      - name: Run current version benchmarks
        run: |
          cd tests
          python regression_test.py --version=current \
            --output=current-performance.json

      - name: Run previous version benchmarks
        run: |
          cd previous/tests
          python regression_test.py --version=previous \
            --output=../previous-performance.json

      - name: Compare performance
        run: |
          cd tests
          python compare_performance.py \
            --current=current-performance.json \
            --previous=../previous-performance.json \
            --output=performance-comparison.html \
            --threshold=10  # 10% regression threshold

      - name: Upload regression results
        uses: actions/upload-artifact@v3
        with:
          name: regression-results
          path: |
            tests/current-performance.json
            previous-performance.json
            tests/performance-comparison.html

      - name: Comment on PR (if regression detected)
        if: github.event_name == 'push' && github.event.pull_request
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');

            // Read performance comparison results
            const comparisonFile = 'tests/performance-comparison.json';
            if (fs.existsSync(comparisonFile)) {
              const comparison = JSON.parse(fs.readFileSync(comparisonFile, 'utf8'));
              
              if (comparison.hasRegression) {
                const comment = `
                ## âš ï¸ Performance Regression Detected
                
                The following performance regressions were detected in this commit:
                
                ${comparison.regressions.map(r => `- **${r.metric}**: ${r.change}% slower`).join('\n')}
                
                **Details:**
                - Current performance: [View Results](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})
                - Threshold: 10% regression
                
                Please review the changes and consider optimizations.
                `;
                
                github.rest.issues.createComment({
                  issue_number: context.issue.number,
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  body: comment
                });
              }
            }

  # =============================================================================
  # PERFORMANCE DASHBOARD
  # =============================================================================

  performance-dashboard:
    name: Performance Dashboard
    runs-on: ubuntu-latest
    needs:
      [load-testing, benchmark-testing, memory-profiling, scalability-testing]
    if: always()
    steps:
      - name: Download all artifacts
        uses: actions/download-artifact@v3

      - name: Generate performance dashboard
        run: |
          python tests/generate_performance_dashboard.py \
            --load-results=performance-results-*/  \
            --benchmark-results=benchmark-results/ \
            --memory-results=memory-profiling-results/ \
            --scalability-results=scalability-results/ \
            --output=performance-dashboard.html

      - name: Upload performance dashboard
        uses: actions/upload-artifact@v3
        with:
          name: performance-dashboard
          path: performance-dashboard.html

      - name: Generate performance summary
        run: |
          echo "## ðŸ“Š Performance Test Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Test Date:** $(date -u)" >> $GITHUB_STEP_SUMMARY
          echo "**Repository:** ${{ github.repository }}" >> $GITHUB_STEP_SUMMARY
          echo "**Commit:** ${{ github.sha }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Test Type | Status | Details |" >> $GITHUB_STEP_SUMMARY
          echo "|-----------|--------|---------|" >> $GITHUB_STEP_SUMMARY
          echo "| Load Testing | ${{ needs.load-testing.result }} | API, Pipeline, Stress tests |" >> $GITHUB_STEP_SUMMARY
          echo "| Benchmarking | ${{ needs.benchmark-testing.result }} | Go & Python benchmarks |" >> $GITHUB_STEP_SUMMARY
          echo "| Memory Profiling | ${{ needs.memory-profiling.result }} | Memory usage analysis |" >> $GITHUB_STEP_SUMMARY
          echo "| Scalability | ${{ needs.scalability-testing.result }} | Horizontal scaling tests |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Count successful tests
          SUCCESS_COUNT=0
          TOTAL_COUNT=4

          [[ "${{ needs.load-testing.result }}" == "success" ]] && ((SUCCESS_COUNT++))
          [[ "${{ needs.benchmark-testing.result }}" == "success" ]] && ((SUCCESS_COUNT++))
          [[ "${{ needs.memory-profiling.result }}" == "success" ]] && ((SUCCESS_COUNT++))
          [[ "${{ needs.scalability-testing.result }}" == "success" ]] && ((SUCCESS_COUNT++))

          echo "**Performance Score:** $SUCCESS_COUNT/$TOTAL_COUNT tests passed" >> $GITHUB_STEP_SUMMARY

          if [ $SUCCESS_COUNT -eq $TOTAL_COUNT ]; then
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "âœ… **All performance tests passed!** System performance is optimal." >> $GITHUB_STEP_SUMMARY
          elif [ $SUCCESS_COUNT -ge 2 ]; then
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "ðŸŸ¡ **Most performance tests passed.** Some optimizations may be needed." >> $GITHUB_STEP_SUMMARY
          else
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "âŒ **Performance issues detected.** Please review and optimize the system." >> $GITHUB_STEP_SUMMARY
          fi

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "ðŸ“ˆ **View detailed dashboard:** [Performance Results](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})" >> $GITHUB_STEP_SUMMARY

  # =============================================================================
  # PERFORMANCE ALERTS
  # =============================================================================

  performance-alerts:
    name: Performance Alerts
    runs-on: ubuntu-latest
    needs: [performance-dashboard]
    if: failure()
    steps:
      - name: Send performance alert
        uses: 8398a7/action-slack@v3
        with:
          status: "warning"
          text: |
            ðŸ“ˆ PERFORMANCE ALERT - Performance degradation detected
            Repository: ${{ github.repository }}
            Commit: ${{ github.sha }}
            Details: https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.PERFORMANCE_SLACK_WEBHOOK_URL }}
